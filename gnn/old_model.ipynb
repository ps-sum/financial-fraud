{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max transactions:  181\n",
      "Min transactions:  1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8288 entries, 0 to 8287\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   source  8288 non-null   float64\n",
      " 1   target  8288 non-null   float64\n",
      " 2   weight  8288 non-null   float64\n",
      " 3   label   8288 non-null   float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 323.8 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the environment variable before importing TensorFlow and TensorFlow GNN\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "#  Import networkX graph from file\n",
    "G = nx.read_graphml('data/graph.graphml')\n",
    "\n",
    "# Extract node indices\n",
    "node_list = list(G.nodes)\n",
    "node_indices = {node: idx for idx, node in enumerate(node_list)}\n",
    "\n",
    "# Calculate edge weights\n",
    "max_transactions = max([attrs[\"total_transactions\"] for _, _, attrs in G.edges(data=True)])\n",
    "min_transactions = min([attrs[\"total_transactions\"] for _, _, attrs in G.edges(data=True)])\n",
    "\n",
    "print(\"Max transactions: \", max_transactions)\n",
    "print(\"Min transactions: \", min_transactions)\n",
    "\n",
    "# Max min regulzarization\n",
    "for u, v, attrs in G.edges(data=True):\n",
    "    attrs[\"weight\"] = (attrs[\"total_transactions\"] - min_transactions) / (max_transactions - min_transactions)\n",
    "\n",
    "\n",
    "# Extract edge indices using the node indices\n",
    "edge_indices = np.array([(node_indices[u], node_indices[v]) for u, v, attrs in G.edges(data=True)])\n",
    "edge_weights = np.array([attrs[\"weight\"] for _, _, attrs in G.edges(data=True)])\n",
    "\n",
    "labels = np.array([0 if attrs[\"fraud_proportion\"] == 0 else 1 for _, _, attrs in G.edges(data=True)])\n",
    "\n",
    "# Create a DataFrame with the edge indices\n",
    "adjency_matrix_coo_df = pd.DataFrame([], columns=[\"source\", \"target\", \"weight\", \"label\"])\n",
    "for ix, edge in enumerate(edge_indices):\n",
    "    adjency_matrix_coo_df.loc[len(adjency_matrix_coo_df)] = [edge[0], edge[1], edge_weights[ix], labels[ix]]\n",
    "\n",
    "\n",
    "adjency_matrix_coo_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Graph to GNN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 8288\n",
      "Number of nodes: 7311\n"
     ]
    }
   ],
   "source": [
    "# How many edges?\n",
    "print('Number of edges:', G.number_of_edges())\n",
    "\n",
    "# How many nodes?\n",
    "print('Number of nodes:', G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country vocab size: 1\n",
      "Type vocab size: 1\n",
      "Customer ID vocab size: 7311\n",
      "(7311, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "country_vocab = set()\n",
    "account_type_vocab = set()\n",
    "customer_id_vocab = set()\n",
    "\n",
    "for n, attrs in G.nodes(data=True):\n",
    "\n",
    "    # Preprocess the customer ID\n",
    "    attrs[\"customer_id\"] = attrs.get(\"customer_id\").replace(\"_\", \"\")\n",
    "\n",
    "    country_vocab.add(attrs.get(\"country\", \"unknown\"))\n",
    "    account_type_vocab.add(attrs.get(\"type\", \"unknown\"))\n",
    "    customer_id_vocab.add(attrs.get(\"customer_id\"))\n",
    "\n",
    "print(f\"Country vocab size: {len(country_vocab)}\")\n",
    "print(f\"Type vocab size: {len(account_type_vocab)}\")\n",
    "print(f\"Customer ID vocab size: {len(customer_id_vocab)}\")\n",
    "\n",
    "\n",
    "# Define a TensorFlow encoder\n",
    "class NodeAttributeEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, country_vocab_size, type_vocab_size, customer_id_vocab_size, output_dim\n",
    "    ):\n",
    "        super(NodeAttributeEncoder, self).__init__()\n",
    "        self.country_embedding = tf.keras.layers.Embedding(\n",
    "            country_vocab_size, output_dim\n",
    "        )\n",
    "        self.account_type_embedding = tf.keras.layers.Embedding(\n",
    "            type_vocab_size, output_dim\n",
    "        )\n",
    "        self.customer_id_normalizer = tf.keras.layers.Embedding(\n",
    "            customer_id_vocab_size, output_dim\n",
    "        )\n",
    "\n",
    "        # Initialize tokenizers\n",
    "        self.country_tokenizer = Tokenizer(\n",
    "            num_words=country_vocab_size, oov_token=\"<OOV>\", split=\" \"\n",
    "        )\n",
    "        self.type_tokenizer = Tokenizer(num_words=type_vocab_size, oov_token=\"<OOV>\", split=\" \")\n",
    "        self.customer_id_tokenizer = Tokenizer(\n",
    "            num_words=customer_id_vocab_size, oov_token=\"<OOV>\", split=\" \"\n",
    "        )\n",
    "\n",
    "    def fit_tokenizers(self, country_texts, type_texts, customer_id_texts):\n",
    "        # Fit tokenizers on the respective input data\n",
    "        self.country_tokenizer.fit_on_texts(country_texts)\n",
    "        self.type_tokenizer.fit_on_texts(type_texts)\n",
    "        self.customer_id_tokenizer.fit_on_texts(customer_id_texts)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        country, customer_id, account_type_ = inputs\n",
    "\n",
    "        # Convert text to sequences of integers\n",
    "        country_seq = self.country_tokenizer.texts_to_sequences([country])\n",
    "        type_seq = self.type_tokenizer.texts_to_sequences([account_type_])\n",
    "        customer_id_seq = self.customer_id_tokenizer.texts_to_sequences([customer_id])\n",
    "\n",
    "        # Convert sequences to tensors\n",
    "        country_seq = tf.constant(country_seq)\n",
    "        type_seq = tf.constant(type_seq)\n",
    "        customer_id_seq = tf.constant(customer_id_seq)\n",
    "\n",
    "        # Pad sequences to ensure uniform length\n",
    "        country_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            country_seq, padding=\"post\"\n",
    "        )\n",
    "        type_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            type_seq, padding=\"post\"\n",
    "        )\n",
    "        customer_id_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            customer_id_seq, padding=\"post\"\n",
    "        )\n",
    "\n",
    "        # Embed the sequences\n",
    "        country_encoded = self.country_embedding(country_seq)\n",
    "        type_encoded = self.account_type_embedding(type_seq)\n",
    "        customer_id_encoded = self.customer_id_normalizer(customer_id_seq)\n",
    "        return tf.concat([country_encoded, customer_id_encoded, type_encoded], axis=-1)\n",
    "\n",
    "\n",
    "node_encoder = NodeAttributeEncoder(\n",
    "    len(country_vocab) + 1,\n",
    "    len(account_type_vocab) + 1,\n",
    "    len(customer_id_vocab) + 1,\n",
    "    output_dim=1,\n",
    ")\n",
    "node_encoder.fit_tokenizers(\n",
    "    list(country_vocab), list(account_type_vocab), list(customer_id_vocab)\n",
    ")\n",
    "\n",
    "# Encode node attributes to feature vectors\n",
    "node_features = tf.TensorArray(tf.float32, size=G.number_of_nodes())\n",
    "for index, node_obj in enumerate(G.nodes(data=True)):\n",
    "    node, attrs = node_obj\n",
    "    encoded_features = node_encoder.call(\n",
    "        (\n",
    "            attrs.get(\"country\", \"unknown\"),\n",
    "            attrs.get(\"customer_id\", \"unknown\"),\n",
    "            attrs.get(\"type\", \"unknown\"),\n",
    "        )\n",
    "    )\n",
    "    node_features = node_features.write(index, tf.squeeze(encoded_features, axis=0))\n",
    "\n",
    "# Convert TensorArray to Tensor\n",
    "node_features_tf = node_features.stack()\n",
    "node_features_tf = tf.reshape(node_features_tf, (G.number_of_nodes(), 3))\n",
    "print(node_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8288, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "total_amount_vec = [attrs.get(\"total_amount\") for a, b, attrs in G.edges(data=True)]\n",
    "\n",
    "# Convert to NumPy array and reshape\n",
    "total_amount_vec = np.array(total_amount_vec).reshape(-1, 1)\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "if np.any(np.isnan(total_amount_vec)) or np.any(np.isinf(total_amount_vec)):\n",
    "    raise ValueError(\"Data contains NaN or infinite values\")\n",
    "\n",
    "# Alternative normalization using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(total_amount_vec)\n",
    "total_amount_normalized = scaler.transform(total_amount_vec)\n",
    "\n",
    "\n",
    "total_transactions_vec = np.array(\n",
    "    [attrs.get(\"total_transactions\") for a, b, attrs in G.edges(data=True)]\n",
    ").reshape(-1, 1)\n",
    "scaler.fit(total_transactions_vec)\n",
    "total_transactions_normalized = scaler.transform(total_transactions_vec)\n",
    "\n",
    "fraud_proportion_vec = np.array(\n",
    "    [attrs.get(\"fraud_proportion\") for a, b, attrs in G.edges(data=True)]\n",
    ").reshape(-1, 1)\n",
    "scaler.fit(fraud_proportion_vec)\n",
    "fraud_proportion_normalized = scaler.transform(fraud_proportion_vec)\n",
    "\n",
    "\n",
    "edge_features = tf.concat(\n",
    "    [\n",
    "        total_amount_normalized,\n",
    "        # min_amount_normalized,\n",
    "        # max_amount_normalized,\n",
    "        # mean_amount_normalized,\n",
    "        total_transactions_normalized,\n",
    "        fraud_proportion_normalized,\n",
    "    ],\n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "print(edge_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features shape:  (7311, 3)\n",
      "Edge features shape:  (8288, 3)\n",
      "Edge labels shape:  (8288,)\n"
     ]
    }
   ],
   "source": [
    "# Create a GraphTensor\n",
    "from spektral.data.graph import Graph\n",
    "from spektral.data import SingleLoader, Dataset\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "num_nodes = len(G.nodes)\n",
    "\n",
    "source_nodes = adjency_matrix_coo_df[\"source\"].apply(lambda x: int(x)).values\n",
    "target_nodes = adjency_matrix_coo_df[\"target\"].apply(lambda x: int(x)).values\n",
    "weights = adjency_matrix_coo_df[\"weight\"].values\n",
    "\n",
    "a = coo_matrix(\n",
    "    (weights, (source_nodes, target_nodes)),\n",
    "    shape=(num_nodes, num_nodes),\n",
    ")\n",
    "\n",
    "x = node_features_tf.numpy()  # node features\n",
    "e = edge_features.numpy()  # edge features\n",
    "y = adjency_matrix_coo_df[\"label\"].values  # edge labels\n",
    "\n",
    "print(\"Node features shape: \", x.shape)\n",
    "print(\"Edge features shape: \", e.shape)\n",
    "print(\"Edge labels shape: \", y.shape)\n",
    "\n",
    "\n",
    "graph = Graph(x=x, a=a, e=e, y=y)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class SingleGraphDataset(Dataset):\n",
    "    def __init__(self, graph, **kwargs):\n",
    "        self.graph = graph\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        return [self.graph]\n",
    "\n",
    "dataset = SingleGraphDataset(graph)\n",
    "loader = SingleLoader(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext:0' shape=(7311, 3) dtype=float32>, SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"edge_gcn/Cast:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"shape:0\", shape=(2,), dtype=int64)), <tf.Tensor 'edge_gcn/Cast_1:0' shape=(8288, 3) dtype=float32>)\n",
      "(<tf.Tensor 'IteratorGetNext:0' shape=(7311, 3) dtype=float32>, SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"edge_gcn/Cast:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"shape:0\", shape=(2,), dtype=int64)), <tf.Tensor 'edge_gcn/Cast_1:0' shape=(8288, 3) dtype=float32>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:39.898671: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [7311,1] vs. [8288,1]\n",
      "\t [[{{node gradient_tape/binary_crossentropy/logistic_loss/mul/BroadcastGradientArgs}}]]\n"
     ]
    }
   ],
   "source": [
    "from spektral.layers import EdgeConv\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the GCN model\n",
    "class EdgeGCN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = EdgeConv(32, activation='relu')\n",
    "        self.conv2 = EdgeConv(16, activation='relu')\n",
    "        self.dense = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, e = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        x = self.conv2([x, a, e])\n",
    "        # Aggregate edge features\n",
    "        edge_indices = tf.stack([a.indices[:, 0], a.indices[:, 1]], axis=1)\n",
    "        edge_features = tf.gather(x, edge_indices[:, 0]) + tf.gather(x, edge_indices[:, 1])\n",
    "        return self.dense(edge_features)\n",
    "\n",
    "# Create the model\n",
    "model = EdgeGCN()\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(loader.load(), epochs=1, steps_per_epoch=loader.steps_per_epoch)\n",
    "\n",
    "# Predict the fraud proportion\n",
    "y_pred = model.predict(loader.load(), steps=loader.steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43my_pred\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Merge original fraud proportions with predicted fraud proportions\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Merge original fraud proportions with predicted fraud proportions\n",
    "y_pred = np.squeeze(y_pred)\n",
    "y = np.squeeze(y)\n",
    "\n",
    "for original, predicted in sorted(zip(y, y_pred), key=lambda x: x[0], reverse=True):\n",
    "    print(f\"Original: {original} - Predicted: {predicted}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(y, y_pred)\n",
    "plt.xlabel('True fraud proportion')\n",
    "plt.ylabel('Predicted fraud proportion')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffd_gnn-yt_WEb1U",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
